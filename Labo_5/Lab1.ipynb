{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Neural networks\n",
    "\n",
    "\n",
    "These exercises should be done individually (i.e. no plagerism). Discussion is allowed.\n",
    "\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "> Structure the code so you have a function for forward and backpropagation.\n",
    "\n",
    "> By having a function that does the forward pass through the network and another function to do the backpropagation step, neural networks with many layers can be created easily\n",
    "\n",
    "> Use the example code that was given to do this. Make sure to think about what you are doing. This is not just a copy-paste exercise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Benjamin Fraeyman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from NN_Helper import Gradient_Checker\n",
    "gradient_checker = Gradient_Checker(limit=1.0*np.exp(-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create toy dataset\n",
    "\n",
    "> The dataset has the following dimensions $[n,d]$\n",
    "\n",
    "> $n$ is the amount of samples\n",
    "\n",
    "> $d$ is the dimensionality of the data (4 in this case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set containing samples with features\n",
    "\n",
    "#X has 6 samples (n) and each sample has 4 features (d)\n",
    "X = np.array([  [0.,0.,1.,0.],\n",
    "                [0.,1.,1.,0.],\n",
    "                [1.,0.,1.,0.],\n",
    "                [1.,1.,1.,0.],\n",
    "                [0.,0.,1.,1.],\n",
    "                [0.,1.,1.,1.]])\n",
    "\n",
    "# Ground truth\n",
    "y = np.array([  [1.,0.,0.],\n",
    "                [1.,0.,0.],\n",
    "                [0.,1.,0.],\n",
    "                [0.,1.,0.],\n",
    "                [0.,0.,1.],\n",
    "                [0.,0.,1.]])\n",
    "\n",
    "n_samples = float(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weight matrix\n",
    "\n",
    "> The matrix has the following dimensions $[d,k]$\n",
    "\n",
    "> $d$ is the dimensionality of the data (4 in this case)\n",
    "\n",
    "> $k$ is the amount of classes (3 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "w = 2*np.random.random((4,3)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function and the derivative of this function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "        output = 1/(1+np.exp(-x))\n",
    "        return output\n",
    "    \n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_output_to_derivative(output):\n",
    "        return output*(1-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation\n",
    "\n",
    "> The forward step should return the activation of that layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation. (Do not modify the parameters' names)\n",
    "def forward(input_layer=None,weights=None):\n",
    "    p = np.dot(input_layer,weights) # Dimensions X=[6x4], w=[4x3], so p=[6x4].[4x3]= 6x3\n",
    "    a = sigmoid(p) # Dimensions a=[6x3]\n",
    "    return a\n",
    "    # <Fill-in>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "> As in the example, the backpropagation function should return the gradient of the loss function with respect to the weights ($ \\frac{\\partial L}{\\partial W}$).\n",
    "\n",
    "> However, this function should also return the gradient of the loss function with respect to the input of the layer ($ \\frac{\\partial L}{\\partial X}$). This is required for a multi-layer neural network\n",
    "\n",
    "> In order to find out how to do this you will have to make the derivation yourself. \n",
    "\n",
    "> Tip: draw the block diagram of 1 variable passing through the a simple network and calculate the gradient at every step as seen in the theory lesson for $ \\frac{\\partial L}{\\partial W}$\n",
    "\n",
    ">Make sure to do a dimensionality check of the matrices. For example $ \\frac{\\partial L}{\\partial W}$ has the same dimensionality as $ W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backpropagation\n",
    "def backwards(input_layer=None,weights=None, a=None, dlda=None):\n",
    "    dadp = sigmoid_output_to_derivative(a) # Dimensions = [6x3]\n",
    "    dpdw = input_layer.T # Dimensions = [4x6]\n",
    "    #should return dldw and dldx\n",
    "    #dldw = gradient of the loss with respect to the weights\n",
    "    #dldx = gradient of the loss with respect to the input of the layer\n",
    "    dldw = np.dot(dpdw,dlda*dadp) # Dimensions = [4x3] = [4x6].([6x3]*[6x3])\n",
    "    dpdx = weights.T\n",
    "    dldx = np.dot(dlda*dadp, dpdx) # Dimensions = [4x3] = [4x6].([6x3]*[6x3])\n",
    "    return dldw,dldx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function\n",
    "def squared_loss(predicted=None,target=None):\n",
    "    loss = 0.5*np.sum((predicted-target)**2) \n",
    "    loss /= n_samples\n",
    "    return loss\n",
    "\n",
    "#dlda\n",
    "#this function calculates the gradient of the loss function with respect to its input (the activation of the last layer)\n",
    "def squared_loss_derrivative(predicted=None,target=None):\n",
    "        dlda = predicted-target # Dimensions = [6x3]\n",
    "        return dlda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function\n",
    "\n",
    "> In this function the neural network is actually trained. For every epoch, forward propagation is done, the loss is calculated, backpropagation is done and the weights are updated using gradient descent\n",
    "\n",
    "> In the main function the gradient check is also executed. \n",
    "> The difference between the analytical gradient and numerical gradient should be smaller than 1.0 e^-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good gradient, difference is: 2.441662664676149e-09\n",
      "Good gradient, difference is: 6.313606532687318e-10\n",
      "Good gradient, difference is: 2.2309005559501323e-10\n",
      "Good gradient, difference is: 2.6855863152267086e-10\n",
      "Good gradient, difference is: 4.214612606024526e-10\n",
      "Good gradient, difference is: 5.218831076354277e-10\n"
     ]
    }
   ],
   "source": [
    "# A list to store the loss per epoch in. We can plot this later on to see if the network learns something\n",
    "loss_list=[]\n",
    "\n",
    "# How many times we will do the combination of forward and backward propagation\n",
    "n_epoch = 60000\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "for iter in xrange(n_epoch):\n",
    "    #Use your forward function and your backwards function to train the neural network. \n",
    "    #Don't forget to add the calculated loss value to the loss_list \n",
    "    #Make sure your backwards function returns the gradient of the loss function with respect tot the weights (dldw) \n",
    "    #and also the gradient of the loss with respect to the layer's input (dldx)\n",
    "    a = forward(input_layer=X,weights=w)\n",
    "    \n",
    "    loss_list.append(squared_loss(predicted=a,target=y))\n",
    "    \n",
    "    dldw,dldx = backwards(input_layer= X,weights=w, a=a, dlda = squared_loss_derrivative(predicted=a,target=y))\n",
    "    dldw /= n_samples\n",
    "    \n",
    "    # Gradient check.\n",
    "    if iter % 10000 == 0.:\n",
    "        f = lambda x: squared_loss(target=y,predicted=forward(input_layer=X,weights=w))\n",
    "        gradient_checker.gradient_check(X,y,dldx,f)\n",
    "        \n",
    "    #Make sure you update the weight matrix (w) using the gradient descent update rule\n",
    "    w += -learning_rate*dldw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "\n",
      "The output of the network\n",
      "\n",
      "[[0.8496089  0.10712236 0.11145403]\n",
      " [0.86858536 0.08359085 0.08576569]\n",
      " [0.07950405 0.93039245 0.02299345]\n",
      " [0.09177659 0.91041254 0.01729694]\n",
      " [0.07955393 0.02904082 0.93006661]\n",
      " [0.0918334  0.02223432 0.90864648]]\n",
      "\n",
      "The ground truth:\n",
      "\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "\n",
      "Apply argmax on the output to get the index per row where the value is maximum\n",
      "\n",
      "Prediction network\n",
      "\n",
      "[0 0 1 1 2 2]\n",
      "\n",
      "Ground truth\n",
      "\n",
      "[0 0 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "print \"Output After Training:\"\n",
    "print\n",
    "print \"The output of the network\"\n",
    "print\n",
    "#Write down the variable you defined as the output of your network\n",
    "print a\n",
    "print\n",
    "print \"The ground truth:\"\n",
    "print\n",
    "print y\n",
    "print\n",
    "print\n",
    "print \"Apply argmax on the output to get the index per row where the value is maximum\"\n",
    "print\n",
    "print \"Prediction network\"\n",
    "print\n",
    "#Fill in this variable here too\n",
    "print np.argmax(a,axis=1)\n",
    "print\n",
    "print \"Ground truth\"\n",
    "print\n",
    "print np.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "name": "Lab1.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
