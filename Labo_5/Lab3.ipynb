{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Neural networks\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "> Re-use the code of exercise 2 (two layer-NN).\n",
    "\n",
    "> Up untill now, no bias parameter was used, in this exercise you will add a bias parameter. $\\sigma(x.w +b)$\n",
    "\n",
    "> Make sure the bias parameter is also updated using gradient descent (**you will need to calculate $\\frac{\\partial L}{\\partial b}$**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Benjamin Fraeyman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from NN_Helper import Gradient_Checker\n",
    "gradient_checker = Gradient_Checker(limit=1.0*np.exp(-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create toy dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set containing samples with features\n",
    "X = np.array([  [0.,0.,1.,0.],\n",
    "                [0.,1.,1.,0.],\n",
    "                [1.,0.,1.,0.],\n",
    "                [1.,1.,1.,0.],\n",
    "                [0.,0.,1.,1.],\n",
    "                [0.,1.,1.,1.]])\n",
    "\n",
    "# Ground truth\n",
    "y = np.array([  [1.,0.,0.],\n",
    "                [1.,0.,0.],\n",
    "                [0.,1.,0.],\n",
    "                [0.,1.,0.],\n",
    "                [0.,0.,1.],\n",
    "                [0.,0.,1.]])\n",
    "\n",
    "n_samples = float(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "w1 = 2*np.random.random((4,20)) - 1\n",
    "w2 = 2*np.random.random((20,3)) - 1\n",
    "\n",
    "# initialize the bias for every layer\n",
    "b1 = np.zeros((1,20))\n",
    "b2 = np.zeros((1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activiation function and the derivative of this function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "        output = 1/(1+np.exp(-x))\n",
    "        return output\n",
    "    \n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_output_to_derivative(output):\n",
    "        return output*(1-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-use the forward propagation function you wrote in the previous exercise\n",
    "#update it to use a bias\n",
    "def forward(input_layer=None,weights=None,bias=None):\n",
    "    p = np.dot(input_layer,weights) +bias\n",
    "    a = sigmoid(p)\n",
    "    return a\n",
    "\n",
    "# https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-use the backpropagation function you wrote in the previous exercise \n",
    "#update it to also return the gradient of the loss function\n",
    "#with respect to the bias\n",
    "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "def backwards(input_layer=None,weights=None, a=None, dlda=None):\n",
    "    dadp = sigmoid_output_to_derivative(a)\n",
    "    dpdw = input_layer.T\n",
    "    dldw = np.dot(dpdw,dlda*dadp)\n",
    "    \n",
    "    #dz = dlda*(1-dlda)\n",
    "    #dldb = np.sum(dz, axis=0, keepdims=True)\n",
    "    # (a+w) afgeleid naar w => 1\n",
    "    #btemp = np.zeros((1,weights.shape[1]))\n",
    "    dldp = dlda*dadp\n",
    "    ones = np.ones((1, int(n_samples)))\n",
    "    #ones = np.ones((1, 6))\n",
    "    #ones.T\n",
    "    dldb = np.dot(ones,dldp)#dldp)\n",
    "    \n",
    "    dpdx = weights.T\n",
    "    dldx = np.dot(dlda*dadp, dpdx)\n",
    "    \n",
    "    \n",
    "    dldw /= n_samples\n",
    "    dldb /= n_samples\n",
    "    return dldw,dldb,dldx\n",
    "\n",
    "#should return:\n",
    "#Gradient of the loss with respect to the weights - dldw\n",
    "#Gradient of the loss with respect to the bias - dldb\n",
    "#Gradient of the loss with respect to the input of the layer - dldx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-use the loss function you wrote in the previous exercise\n",
    "def squared_loss(predicted=None,target=None):\n",
    "    loss = 0.5*np.sum((predicted-target)**2)\n",
    "    loss /= n_samples\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivative of the loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-use the function to calculate the derivative of the loss function\n",
    "def squared_loss_derrivative(predicted=None,target=None):\n",
    "        dlda = predicted-target\n",
    "        return dlda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function\n",
    "\n",
    "> In the main function the gradient check is also executed. \n",
    "> The difference between the analytical gradient and numerical gradient should be smaller than 1.0 e^-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good gradient, difference is: 1.05410532970701e-09\n",
      "Good gradient, difference is: 6.374744492508185e-10\n",
      "Good gradient, difference is: 1.7406645911953918e-09\n",
      "Good gradient, difference is: 8.361300323349188e-10\n",
      "Good gradient, difference is: 1.6063379760479847e-09\n",
      "Good gradient, difference is: 3.3122433150420643e-09\n"
     ]
    }
   ],
   "source": [
    "# A list to store the loss per epoch in. We can plot this later on to see if the network learns something\n",
    "loss_list=[]\n",
    "\n",
    "# How many times we will do the combination of forward and backward propagation\n",
    "n_epoch = 60000\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "for iter in xrange(n_epoch):\n",
    "    #Use the forwards and backwards function to create a two layer neural network \n",
    "    #wherein each layer uses a bias (b1 and b2 respectively)\n",
    "    #Again make sure to calculate dldx, which is the gradient of the loss function with respect to the input of the network\n",
    "    \n",
    "    a1 = forward(input_layer=X,weights=w1,bias=b1)\n",
    "    a2 = forward(input_layer=a1,weights=w2,bias=b2)\n",
    "    \n",
    "    #Calculate the loss and add it to the loss_list\n",
    "    loss_list.append(squared_loss(predicted=a2,target=y))\n",
    "    \n",
    "    dldw2, dldb2, dldx2 = backwards(input_layer= a1,weights=w2, a=a2, dlda = squared_loss_derrivative(predicted=a2,target=y))\n",
    "    dldw, dldb, dldx = backwards(input_layer= X,weights=w1, a=a1, dlda = dldx2/n_samples)\n",
    "\n",
    "    \n",
    "    # Gradient check.\n",
    "    if iter % 10000 == 0.:\n",
    "        f = lambda x: squared_loss(target=y,predicted=forward(input_layer=forward(input_layer=X,weights=w1,bias=b1),weights=w2,bias=b2))\n",
    "        gradient_checker.gradient_check(X,y,dldx,f)\n",
    "        \n",
    "    #Do not forget to update the weights using the gradient descent update rule\n",
    "    w1 += -learning_rate*dldw\n",
    "    w2 += -learning_rate*dldw2\n",
    "    \n",
    "    #Also update the bias using the gradient descent update rule\n",
    "    b1 += -learning_rate*dldb\n",
    "    b2 += -learning_rate*dldb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "\n",
      "The output of the network\n",
      "\n",
      "[[0.84247073 0.09805121 0.11035624]\n",
      " [0.86516512 0.10291533 0.07693742]\n",
      " [0.09428171 0.89763911 0.0260825 ]\n",
      " [0.12038181 0.88921355 0.02208068]\n",
      " [0.09446223 0.01717599 0.92002539]\n",
      " [0.10240357 0.01953083 0.89414585]]\n",
      "\n",
      "The ground truth:\n",
      "\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "\n",
      "Apply argmax on the output to get the index per row where the value is maximum\n",
      "\n",
      "Prediction network\n",
      "\n",
      "[0 0 1 1 2 2]\n",
      "\n",
      "Ground truth\n",
      "\n",
      "[0 0 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "print \"Output After Training:\"\n",
    "print\n",
    "print \"The output of the network\"\n",
    "print\n",
    "print a2\n",
    "print\n",
    "print \"The ground truth:\"\n",
    "print\n",
    "print y\n",
    "print\n",
    "print\n",
    "print \"Apply argmax on the output to get the index per row where the value is maximum\"\n",
    "print\n",
    "print \"Prediction network\"\n",
    "print\n",
    "print np.argmax(a2,axis=1)\n",
    "print\n",
    "print \"Ground truth\"\n",
    "print\n",
    "print np.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "name": "Lab3.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
