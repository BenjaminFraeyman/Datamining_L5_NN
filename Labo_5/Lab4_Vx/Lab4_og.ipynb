{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Neural networks\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "> In this final exercise you will have to create a neural network to classify hand written numbers. Such a neural network is for example used in the post sorting process.\n",
    "\n",
    "> The dataset consists of images of hand written numbers ranging from 0 till 9.\n",
    "\n",
    "> Your goal is to train a neural network on the training set (*X_train*) and predict on the test_set (*X_test*)\n",
    "\n",
    "> Try to get an accuracy as high as possible. Decide yourself what to modify. You can modify for example: the learning_rate, amount of layers, neurons per layer, preprocessing methods, gradient descent to stochastic gradient descent/ batch gradient descent...\n",
    "\n",
    "> Feel free to change other aspects too if you want (e.g. activation function, but then you have to change to backprop algorithm).\n",
    "\n",
    "#### A few hints:\n",
    "\n",
    "> Calculate the accuracy on both the training set (X_train) and testing set (X_test) to see if there is a large difference\n",
    "\n",
    "> You can also do this every 5,10,... epochs (monitor the accuracy)\n",
    "\n",
    "> Calculate the loss on the training set, but also the test set (monitor the loss)\n",
    "\n",
    "> Plot these\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Benjamin Fraeyman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from NN_Helper import Gradient_Checker\n",
    "gradient_checker = Gradient_Checker(limit=1.0*np.exp(-8))\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create toy dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (1797L, 64L)\n",
      "y.shape: (1797L, 10L)\n",
      "X_train.shape: (1700L, 64L)\n",
      "X_test.shape: (97L, 64L)\n",
      "y_train.shape: (1700L, 10L)\n",
      "y_test.shape: (97L, 10L)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set containing samples with features\n",
    "data = load_digits(10)\n",
    "X = data['data']\n",
    "print \"X.shape:\", X.shape\n",
    "y = data['target']\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "y = encoder.fit_transform(np.reshape(y,(len(y),1))).toarray()\n",
    "print \"y.shape:\", y.shape\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "X_train = X[:1700]\n",
    "print \"X_train.shape:\", X_train.shape\n",
    "X_test = X[1700:]\n",
    "print \"X_test.shape:\", X_test.shape\n",
    "y_train = y[:1700]\n",
    "print \"y_train.shape:\", y_train.shape\n",
    "y_test = y[1700:]\n",
    "print \"y_test.shape:\", y_test.shape\n",
    "\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.imshow(np.reshape(X[i*100],(8,8)),cmap=plt.cm.gray,interpolation='None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-use the code of exercise 3 (backwards, forwards, loss,...)** (you may use more cells if you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- reused code here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 1700.0\n",
      "n_neurons: 1\n",
      "w1.shape: (64L, 1L)\n",
      "w2.shape: (1L, 10L)\n",
      "b1.shape: (1L, 1L)\n",
      "b2.shape: (1L, 10L)\n"
     ]
    }
   ],
   "source": [
    "n_samples = float(len(X_train))\n",
    "print \"n_samples:\", n_samples\n",
    "n_neurons = 1\n",
    "print \"n_neurons:\", n_neurons\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "w1 = 2*np.random.random((X_train.shape[1],n_neurons)) - 1\n",
    "print \"w1.shape:\", w1.shape\n",
    "w2 = 2*np.random.random((n_neurons,y_train.shape[1])) - 1\n",
    "print \"w2.shape:\", w2.shape\n",
    "\n",
    "# initialize the bias for every layer\n",
    "b1 = np.zeros((1,w1.shape[1]))\n",
    "print \"b1.shape:\", b1.shape\n",
    "b2 = np.zeros((1,w2.shape[1]))\n",
    "print \"b2.shape:\", b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activiation function and the derivative of this function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "        output = 1/(1+np.exp(-x))\n",
    "        return output\n",
    "    \n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_output_to_derivative(output):\n",
    "        return output*(1-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-use the forward propagation function you wrote in the previous exercise\n",
    "#update it to use a bias\n",
    "def forward(input_layer=None,weights=None,bias=None):\n",
    "    p = np.dot(input_layer,weights) +bias\n",
    "    a = sigmoid(p)\n",
    "    return a\n",
    "\n",
    "# https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "def backwards(input_layer=None,weights=None, a=None, dlda=None):\n",
    "    dadp = sigmoid_output_to_derivative(a)\n",
    "    \n",
    "    dpdw = input_layer.T\n",
    "    dldw = np.dot(dpdw,dlda*dadp)\n",
    "    \n",
    "    # (a+w) afgeleid naar w => 1\n",
    "    dldp = dlda*dadp\n",
    "    ones = np.ones((1, int(n_samples)))\n",
    "    dldb = np.dot(ones,dldp)\n",
    "    \n",
    "    dpdx = weights.T\n",
    "    dldx = np.dot(dlda*dadp, dpdx)\n",
    "    \n",
    "    dldw /= n_samples\n",
    "    dldb /= n_samples\n",
    "    return dldw,dldb,dldx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(predicted=None,target=None):\n",
    "    loss = 0.5*np.sum((predicted-target)**2)\n",
    "    loss /= n_samples\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivative of the loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss_derrivative(predicted=None,target=None):\n",
    "        dlda = predicted-target\n",
    "        return dlda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- end of reused code --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1.shape: (1700L, 1L)\n",
      "a2.shape: (1700L, 10L)\n",
      "X_train.shape: (1700L, 64L)\n",
      "y_train.shape: (1700L, 10L)\n",
      "dldx.shape: (1700L, 64L)\n",
      "Bad gradient, difference is: 0.002265270793923051\n"
     ]
    }
   ],
   "source": [
    "#<Fill-in>-----------\n",
    "# A list to store the loss per epoch in. We can plot this later on to see if the network learns something\n",
    "loss_list=[]\n",
    "# How many times we will do the combination of forward and backward propagation\n",
    "n_epoch = 1\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "# --------------------\n",
    "\n",
    "for iter in xrange(n_epoch):\n",
    "    #As in the previous exercises, do the forward passes, backward passes, update the weights, \n",
    "    # update the biases, save the loss, ...\n",
    "    a1 = forward(input_layer=X_train,weights=w1,bias=b1)\n",
    "    print \"a1.shape:\", a1.shape\n",
    "    a2 = forward(input_layer=a1,weights=w2,bias=b2)\n",
    "    print \"a2.shape:\", a2.shape\n",
    "    loss_list.append(squared_loss(predicted=a2,target=y_train))\n",
    "    dldw2, dldb2, dldx2 = backwards(input_layer= a1,weights=w2, a=a2, dlda = squared_loss_derrivative(predicted=a2,target=y_train))\n",
    "    dldw, dldb, dldx = backwards(input_layer= X_train,weights=w1, a=a1, dlda = dldx2/n_samples)\n",
    "    if iter % 1 == 0.:\n",
    "        f = lambda x: squared_loss(target=y_train,predicted=forward(input_layer=forward(input_layer=X_train,weights=w1,bias=b1),weights=w2,bias=b2))\n",
    "        print \"X_train.shape:\", X_train.shape\n",
    "        print \"y_train.shape:\", y_train.shape\n",
    "        print \"dldx.shape:\", dldx.shape\n",
    "        gradient_checker.gradient_check(X_train,y_train,dldx,f)\n",
    "    w1 += -learning_rate*dldw\n",
    "    w2 += -learning_rate*dldw2\n",
    "    b1 += -learning_rate*dldb\n",
    "    b2 += -learning_rate*dldb2\n",
    "\n",
    "\n",
    "# for iter in xrange(n_epoch):\n",
    "#     #As in the previous exercises, do the forward passes, backward passes, update the weights, \n",
    "#     # update the biases, save the loss, ...\n",
    "#     a1 = forward(input_layer=X,weights=w1,bias=b1)\n",
    "#     a2 = forward(input_layer=a1,weights=w2,bias=b2)\n",
    "#     loss_list.append(squared_loss(predicted=a2,target=y))\n",
    "#     dldw2, dldb2, dldx2 = backwards(input_layer= a1,weights=w2, a=a2, dlda = squared_loss_derrivative(predicted=a2,target=y))\n",
    "#     dldw, dldb, dldx = backwards(input_layer= X,weights=w1, a=a1, dlda = dldx2/n_samples)\n",
    "#     if iter % 1 == 0.:\n",
    "#         f = lambda x: squared_loss(target=y,predicted=forward(input_layer=forward(input_layer=X,weights=w1,bias=b1),weights=w2,bias=b2))\n",
    "#         gradient_checker.gradient_check(X,y,dldx,f)\n",
    "#     w1 += -learning_rate*dldw\n",
    "#     w2 += -learning_rate*dldw2\n",
    "#     b1 += -learning_rate*dldb\n",
    "#     b2 += -learning_rate*dldb2\n",
    "    \n",
    "# #Calculate the accuracy for X_train using the \"accuracy_score\" function from scikit-learn which is already imported \n",
    "# # (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "# <Fill-in>\n",
    "# #Calculate the accuracy for X_test using \"the accuracy_score\"\n",
    "# <Fill-in>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Output After Training:\"\n",
    "print\n",
    "print \"The output of the network\"\n",
    "print\n",
    "print a2\n",
    "print\n",
    "print \"The ground truth:\"\n",
    "print\n",
    "print y_train\n",
    "print\n",
    "print\n",
    "print \"Apply argmax on the output to get the index per row where the value is maximum\"\n",
    "print\n",
    "print \"Prediction network\"\n",
    "print\n",
    "print np.argmax(a2,axis=1)\n",
    "print\n",
    "print \"Ground truth\"\n",
    "print\n",
    "print np.argmax(y_train,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question: What is the accuracy you achieved on X_test?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Fill - in>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What modifications did you do to receive this score?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "name": "Lab4.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
